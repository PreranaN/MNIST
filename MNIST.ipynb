{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition.The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. We will build a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7bbe9dd9520e>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Vivek\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "#clears the memory of all variables left from previous runs(reset the computational graph)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1)+ biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2)+ biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3)+ biases_3\n",
    "\n",
    "# Calculate the loss function for every output/target pair.\n",
    "# The function used is the same as applying softmax to the last layer and then calculating cross entropy with the function\n",
    "# Logits here means: unscaled probabilities (so, the outputs, before they are scaled by the softmax)\n",
    "# the labels are the targets.\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Get the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Mean loss: 0.426. Validation loss: 0.211. Validation accuracy: 93.88%\n",
      "Epoch 2. Mean loss: 0.191. Validation loss: 0.159. Validation accuracy: 95.32%\n",
      "Epoch 3. Mean loss: 0.143. Validation loss: 0.128. Validation accuracy: 96.36%\n",
      "Epoch 4. Mean loss: 0.114. Validation loss: 0.112. Validation accuracy: 96.70%\n",
      "Epoch 5. Mean loss: 0.095. Validation loss: 0.106. Validation accuracy: 96.82%\n",
      "Epoch 6. Mean loss: 0.082. Validation loss: 0.101. Validation accuracy: 96.90%\n",
      "Epoch 7. Mean loss: 0.070. Validation loss: 0.098. Validation accuracy: 97.02%\n",
      "Epoch 8. Mean loss: 0.063. Validation loss: 0.090. Validation accuracy: 97.14%\n",
      "Epoch 9. Mean loss: 0.055. Validation loss: 0.095. Validation accuracy: 97.20%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "# Declare the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 100\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Set a miximum number of epochs.\n",
    "max_epochs = 15\n",
    "\n",
    "# Keep track of the validation loss of the previous epoch.\n",
    "# If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "# We initially set it at some arbitrarily high number to make sure we don't trigger it\n",
    "# at the first epoch\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "\n",
    "# Create a loop for the epochs. Epoch_counter is a variable which automatically starts from 0.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # We want to find the average batch losses over the whole epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, get the validation loss and accuracy\n",
    "    # Get the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Print statistics for the current epoch\n",
    "    # Epoch counter + 1, because epoch_counter automatically starts from 0, instead of 1\n",
    "    # We format the losses with 3 digits after the dot\n",
    "    # We format the accuracy in percentages for easier interpretation\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.11%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "# Test accuracy is a list with 1 value, so we want to extract the value from it, using x[0]\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Print the test accuracy formatted in percentages\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
